# Random Forest Classifier
In Random Forest, multiple decision tree algorithms are run instead of just one. To classify a new attribute-based object, each decision tree gives a classification, and finally the decision with the most “votes” is the algorithm's prediction. For regression, the average of the outputs (predictions) of all the trees is taken.

## Steps:  
### 1. Import libraries.
~~~
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}
~~~

### 2. Import a Spark Session.  
~~~
import org.apache.spark.sql.SparkSession
~~~

### 3. Create a Spark session.  
~~~
  def main(): Unit = {
    val spark = SparkSession.builder.appName("RandomForestClassifierExample").getOrCreate()
~~~

### 4. Load and parse the data The characteristics are indexed within the features column, and a maximum of 4 categories are established, from which the values will be treated as continuous.file, converting it to a DataFrame.
~~~
    val data = spark.read.format("libsvm").load("sample_libsvm_data.txt")
~~~

### 5. Index labels, adding metadata to the label column.  
* Fit on whole dataset to include all labels in index.
~~~
    val labelIndexer = new StringIndexer().setInputCol("label").setOutputCol("indexedLabel").fit(data)
~~~
> The `labelIndexer` variable is created to save the indexing of the labels to which we add metadata to pass them from text to numeric type, and then adjust them to the entire dataset (data).

### 6. Automatically identify categorical features, and index them.
* Set maxCategories so features with > 4 distinct values are treated as continuous.
~~~
    val featureIndexer = new VectorIndexer().setInputCol("features").setOutputCol("indexedFeatures").setMaxCategories(4).fit(data)
~~~
>  The characteristics are indexed within the features column, and a maximum of 4 categories are established, from which the values will be treated as continuous.  

### 7. Split the data into training and test sets.  
~~~
    val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))
~~~
> The data is divided into 70% for training and 30% for testing. 

### 8. Train a RandomForest model.  
~~~
    val rf = new RandomForestClassifier().setLabelCol("indexedLabel").setFeaturesCol("indexedFeatures").setNumTrees(10)
~~~
> The `RandomForestClassifier ()` function is used to train the model, using the indexed columns (label and features), and the total number of trees to be generated is indicated (in this case 10, which is the default value).

### 9. Convert indexed labels back to original labels.  
~~~
    val labelConverter = new IndexToString().setInputCol("prediction").setOutputCol("predictedLabel").setLabels(labelIndexer.labels)
~~~
>  The `IndexToString ()` function is used to convert the indexed labels back to the original text values, and they are stored inside the 'prediction' and 'predictedLabel' columns. 

### 10. Chain indexers and forest in a Pipeline.  
~~~
    val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))
~~~

> It binds everything previously done inside a pipeline: the indexers, the model (rf) and the result of the `IndexToString` function (labelConverter).

### 11. Train model. 
~~~
    val model = pipeline.fit(trainingData)
~~~
> The model is trained using the training data.

### 12. Make predictions.  
~~~
    val predictions = model.transform(testData)
~~~
>  Model predictions are calculated and training data is transformed into test data. 

### 13. Select example rows to display.  
~~~
    predictions.select("predictedLabel", "label", "features").show(5)
~~~
> The columns "predictedLabel", "label" and "features" are selected to display in console the first 5 rows of the predictions made by the model.

### 14. Select (prediction, true label) and compute test error.  
~~~
    val evaluator = new MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName("accuracy")

    val accuracy = evaluator.evaluate(predictions)
    println(s"Test Error = ${(1.0 - accuracy)}")
~~~
> The level of model accuracy is calculated based on the predictions obtained (`evaluate` function).
> The error percentage is calculated by subtracting: 1.0 minus the level of accuracy. 

### 15. Print the trees obtained from the model (10).
~~~
    val rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]
    println(s"Learned classification forest model:\n ${rfModel.toDebugString}")

    spark.stop()
  }

main()
~~~

> All the trees generated by the model are printed, in this case 10, along with their resulting conditions and predictions for each of its branches.
> This time an approximate 3% error was obtained, which leaves us with a 97% level of accuracy for the predictions made by the model. This means that the performance level of the model is very good and therefore the results are reliable and largely accurate.  

